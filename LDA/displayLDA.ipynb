{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: \n",
      "438.4093724240194*拍拍贷法务部 + 155.54185297897166*抱歉 + 35.11733078845706*打错 + 29.358144580075127*一下张 + 17.197861065603895*杨佳伟 + 15.99814427660592*微信 + 15.36277208275262*号码 + 15.19660423933441*李涛 + 13.197078616482974*王志伟 + 11.464812992719823*王家\n",
      "topic 1: \n",
      "311.42318451382255*贷款 + 259.66048658398074*转告 + 199.54998518338152*拍拍贷法务部 + 158.42446697114812*派出所 + 147.03307346881704*告诉 + 146.0033710332369*签收 + 142.91087481943958*欠款 + 137.5751711386423*律师 + 131.80858340465423*信函 + 126.45198969060412*户籍地\n",
      "topic 2: \n",
      "1076.4621358228617*贷款 + 628.0508525805459*转告 + 434.9715737057026*派出所 + 392.1309542011366*拍拍贷法务部 + 383.02713613920304*签收 + 377.8413711279729*告诉 + 361.6038575725574*号码 + 339.9443595585528*户籍地 + 321.8209785230032*律师 + 314.5881210417666*信函\n",
      "topic 3: \n",
      "247.3634631827102*欠款 + 161.34152769279572*理解 + 148.11747449511694*还款 + 146.90385594601398*时间 + 145.09895497261192*块钱 + 130.7309783438638*电话 + 102.40087073320235*一共 + 99.19229514138974*金额 + 97.45837428248458*情况 + 90.18975557485541*借款\n",
      "topic 4: \n",
      "112.18791310595658*号码 + 80.00925841012936*贷款 + 52.84674827041647*留在 + 51.76137721292716*拍拍贷法务部 + 41.36240495253714*联系人 + 31.312695435892795*电话 + 27.819249769581525*女儿 + 27.45892619034055*欠款 + 20.196768491840984*别克 + 20.00985241515494*情况\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://172.20.7.199:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.20.7.199 - - [17/Jan/2019 16:35:35] \"GET / HTTP/1.1\" 200 -\n",
      "172.20.7.199 - - [17/Jan/2019 16:35:35] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "172.20.7.199 - - [17/Jan/2019 16:35:35] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "172.20.7.199 - - [17/Jan/2019 16:35:35] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "base_dir = './data'\n",
    "files = sorted([os.path.join(base_dir, file) for file in os.listdir(base_dir) if file.endswith('.csv')])\n",
    "\n",
    "filename = files[0]\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "suggest_words = ['拍拍贷', '上海拍拍贷', '合肥拍拍贷', '长沙拍拍贷', '拍拍贷法务部']\n",
    "for word in suggest_words:\n",
    "    jieba.suggest_freq(word, True)\n",
    "\n",
    "df['content_cut_words'] = df['content'].map(lambda s: ' '.join(jieba.cut(s)))\n",
    "\n",
    "stopwords = []\n",
    "for word in open('stopwords.txt', encoding='utf8', mode='r'):\n",
    "    stopwords.append(word.strip())\n",
    "\n",
    "corpus = df['content_cut_words'].values\n",
    "\n",
    "n_features = 1000\n",
    "cntVector = CountVectorizer(strip_accents='unicode',\n",
    "                            stop_words=stopwords,\n",
    "                            max_features=n_features)\n",
    "cntTf = cntVector.fit_transform(corpus)\n",
    "featureNames = cntVector.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5,\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0,\n",
    "                                learning_method='batch')\n",
    "docres = lda.fit_transform(cntTf)\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print('topic %s: ' % idx)\n",
    "    wordDist = sorted([(i, v) for i, v in enumerate(topic)], key=lambda x: x[1], reverse=True)\n",
    "    top10 = ' + ' .join([str(value) + '*' + featureNames[i] for (i, value) in wordDist[:10]])\n",
    "    print(top10)\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, cntTf, cntVector)\n",
    "data = pyLDAvis.sklearn.prepare(lda, cntTf, cntVector)\n",
    "pyLDAvis.show(data, open_browser=True, ip='172.20.7.199', port=8888)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
